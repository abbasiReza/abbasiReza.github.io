---
---


@article{abbasideciphering,
  abbr = {ECCV 2024},
  title={Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models},
  keyword = {conference},
  author={Abbasi, Reza and Rohban, Mohammad Hossein and Baghshah, Mahdieh Soleymani}
}

@inproceedings{yu2024latentconceptbasedexplanationnlp,
      abbr = {EMNLP 2024},
      bibtex_show={true},
      title={Latent Concept-based Explanation of NLP Models}, 
      author={Xuemin Yu and Fahim Dalvi and Nadir Durrani and Marzia Nouri and Hassan Sajjad},
      booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024)},
      year={2024},
      url={https://arxiv.org/abs/2404.12545},
      archivePrefix={arXiv},
      eprint={2404.12545},
      primaryClass={cs.CL},
      arxiv={2404.12545},
      pdf = {2404.12545v2.pdf},
      keyword = {conference},
      abstract = {Interpreting and understanding the predictions made by deep learning models poses a formidable challenge due to their inherently opaque nature. Many previous efforts aimed at explaining these predictions rely on input features, specifically, the words within NLP models. However, such explanations are often less informative due to the discrete nature of these words and their lack of contextual verbosity. To address this limitation, we introduce the Latent Concept Attribution method (LACOAT), which generates explanations for predictions based on latent concepts. Our foundational intuition is that a word can exhibit multiple facets, contingent upon the context in which it is used. Therefore, given a word in context, the latent space derived from our training process reflects a specific facet of that word. LACOAT functions by mapping the representations of salient input words into the training latent space, allowing it to provide latent context-based explanations of the prediction.}
}



@article{bahmani2024linguistic,
  title={Linguistic Resources and Transformer-based Models for the Machine Translations between Luri and Yazdi Dialects versus Standard Persian},
  author={Bahmani, Zahra and Mirbeygi, Mohaddeseh and Hashemi Dijujin, Negin and Nouri, Marzia and Amani, Mahsa and Asgari, Ehsan and Soleymani Baghshah, Mahdieh and Beigy, Hamid and Movaghar, Ali and Moghimi, Afzal},
  journal={Language and Linguistics},
  volume={19},
  number={37},
  pages={153--172},
  year={2022},
  publisher={Linguistics Society of Iran},
  code = {},
  keyword = {journal},
  abstract = {Despite recent advances in developing language technologies for the standard Persian dialect, the official Iranian language, a large number of Iranian language variations remained computationally unexplored. Iranian languages, e.g., Kurdi, Azeri, and many Persian dialects are examples of low-resource language distinctions lacking significant linguistic resources such as machine-readable lexicons or part-of-speech (POS) taggers. Efforts in developing language technologies for such languages can significantly contribute to language survival in the digital era and promote cultural diversity. To the best of our knowledge, for the first time, we created linguistic resources for the Luri and the Yazdi dialects by introducing the first parallel corpora between these language variations and the modern Persian language. In this study, we train neural encoder-decoders (1) recurrent sequence-to-sequence and (2) transformer-based machine translation models and evaluate the trained model using BLEU score on an unseen test dataset.Availability of datasets and models: Datasets are available here at https://github.com/language-ml/dataset_yazdi_luri.git}
}
